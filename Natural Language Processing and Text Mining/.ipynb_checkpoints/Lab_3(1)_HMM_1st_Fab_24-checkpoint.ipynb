{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84591b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[')', ',', '.', ':', 'ADJ', 'ADV', 'CC', 'CD', 'DT', 'FW', 'IN', 'MD', 'N', 'POS', 'PRO', 'TO', 'V', 'WDT', 'WP', '``']\n",
      "{'DT': 19, 'N': 17, 'PRO': 3, 'CD': 7, ',': 1, 'ADV': 1, 'ADJ': 3}\n",
      "{'V': 2}\n",
      "i= 1  and output= V\n",
      "i= 2  and output= ADJ\n",
      "Fraction of errors (Baseline) : 0.0\n",
      "Fraction of errors (Viterbi): 0.0\n",
      "Tags suggested by Baseline Algorithm: ['N', 'V', 'ADJ', 'N']\n",
      "Tags suggested by Viterbi Algorithm: ['N', 'V', 'ADJ', 'N']\n",
      "Correct tags: ['N', 'V', 'ADJ', 'N']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division #To avoid integer division\n",
    "from operator import itemgetter\n",
    "###Training Phase###\n",
    "\n",
    "with open(\"wsj_training.txt\", \"r\") as myfile:\n",
    "    tr_str = myfile.read()\n",
    "tr_li = tr_str.split()\n",
    "num_words_train = len(tr_li)\n",
    "\n",
    "\n",
    "train_li_words = ['']\n",
    "train_li_words*= num_words_train\n",
    "\n",
    "train_li_tags = ['']\n",
    "train_li_tags*= num_words_train\n",
    "\n",
    "noun_reduced_list = ['NN','NNS','NNP','NNPS']\n",
    "verb_reduced_list = ['VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "adjec_reduced_list = ['JJ', 'JJR', 'JJS']\n",
    "adv_reduced_list = ['RB', 'RBR', 'RBS']\n",
    "pronoun_reduced_list = ['PRP', 'PRP$', 'RP']\n",
    "\n",
    "for i in range(num_words_train):\n",
    "    temp_li = tr_li[i].split(\"/\")\n",
    "    train_li_words[i] = temp_li[0]\n",
    "    if temp_li[1] in noun_reduced_list:\n",
    "        train_li_tags[i] = 'N'\n",
    "    elif temp_li[1] in verb_reduced_list:\n",
    "        train_li_tags[i] = 'V'\n",
    "    elif temp_li[1] in adjec_reduced_list:\n",
    "        train_li_tags[i] = 'ADJ'\n",
    "    elif temp_li[1] in adv_reduced_list:\n",
    "        train_li_tags[i] = 'ADV'        \n",
    "    elif temp_li[1] in pronoun_reduced_list:\n",
    "        train_li_tags[i] = 'PRO'\n",
    "    else:\n",
    "        train_li_tags[i] = temp_li[1]\n",
    "\n",
    "k = sorted(list(set(train_li_tags)))\n",
    "print (k)\n",
    "dict2_tag_follow_tag_ = {}\n",
    "\"\"\"Nested dictionary to store the transition probabilities\n",
    "each tag A is a key of the outer dictionary\n",
    "the inner dictionary is the corresponding value\n",
    "The inner dictionary's key is the tag B following A\n",
    "and the corresponding value is the number of times B follows A\n",
    "\"\"\"\n",
    "\n",
    "dict2_word_tag = {}\n",
    "\"\"\"Nested dictionary to store the emission probabilities.\n",
    "Each word W is a key of the outer dictionary\n",
    "The inner dictionary is the corresponding value\n",
    "The inner dictionary's key is the tag A of the word W\n",
    "and the corresponding value is the number of times A is a tag of W\n",
    "\"\"\"\n",
    "\n",
    "dict_word_tag_baseline = {}\n",
    "#Dictionary with word as key and its most frequent tag as value\n",
    "\n",
    "for i in range(num_words_train-1):\n",
    "    outer_key = train_li_tags[i]\n",
    "    inner_key = train_li_tags[i+1]\n",
    "    dict2_tag_follow_tag_[outer_key]=dict2_tag_follow_tag_.get(outer_key,{})\n",
    "    dict2_tag_follow_tag_[outer_key][inner_key] = dict2_tag_follow_tag_[outer_key].get(inner_key,0)\n",
    "    dict2_tag_follow_tag_[outer_key][inner_key]+=1\n",
    "\n",
    "    outer_key = train_li_words[i]\n",
    "    inner_key = train_li_tags[i]\n",
    "    dict2_word_tag[outer_key]=dict2_word_tag.get(outer_key,{})\n",
    "    dict2_word_tag[outer_key][inner_key] = dict2_word_tag[outer_key].get(inner_key,0)\n",
    "    dict2_word_tag[outer_key][inner_key]+=1\n",
    "\n",
    "\n",
    "\"\"\"The 1st token is indicated by being the 1st word of a senetence, that is the word after period(.)\n",
    "Adjusting for the fact that the first word of the document is not accounted for that way\n",
    "\"\"\"\n",
    "\n",
    "dict2_tag_follow_tag_['.'] = dict2_tag_follow_tag_.get('.',{})\n",
    "dict2_tag_follow_tag_['.'][train_li_tags[0]] = dict2_tag_follow_tag_['.'].get(train_li_tags[0],0)\n",
    "dict2_tag_follow_tag_['.'][train_li_tags[0]]+=1\n",
    "\n",
    "\n",
    "print (dict2_tag_follow_tag_['IN'])\n",
    "print (dict2_word_tag['made'])\n",
    "\n",
    "last_index = num_words_train-1\n",
    "\n",
    "#Accounting for the last word-tag pair\n",
    "outer_key = train_li_words[last_index]\n",
    "inner_key = train_li_tags[last_index]\n",
    "dict2_word_tag[outer_key]=dict2_word_tag.get(outer_key,{})\n",
    "dict2_word_tag[outer_key][inner_key] = dict2_word_tag[outer_key].get(inner_key,0)\n",
    "dict2_word_tag[outer_key][inner_key]+=1\n",
    "\n",
    "\n",
    "\"\"\"Converting counts to probabilities in the two nested dictionaries\n",
    "& also converting the nested dictionaries to outer dictionary with inner sorted lists\n",
    "\"\"\"\n",
    "for key in dict2_tag_follow_tag_:\n",
    "    di = dict2_tag_follow_tag_[key]\n",
    "    s = sum(di.values())\n",
    "    for innkey in di:\n",
    "        di[innkey] /= s\n",
    "    di = di.items()\n",
    "    di = sorted(di,key=lambda x: x[0])\n",
    "    dict2_tag_follow_tag_[key] = di\n",
    "\n",
    "for key in dict2_word_tag:\n",
    "    di = dict2_word_tag[key]\n",
    "    dict_word_tag_baseline[key] = max(di, key=di.get)\n",
    "    s = sum(di.values())\n",
    "    for innkey in di:\n",
    "        di[innkey] /= s\n",
    "    di = di.items()\n",
    "    di = sorted(di,key=lambda x: x[0])\n",
    "    dict2_word_tag[key] = di\n",
    "\n",
    "\n",
    "\n",
    "###Testing Phase###    \n",
    "\n",
    "with open(\"test.txt\", \"r\") as myfile:\n",
    "    te_str = myfile.read()\n",
    "\n",
    "te_li = te_str.split()\n",
    "num_words_test = len(te_li)\n",
    "\n",
    "test_li_words = ['']\n",
    "test_li_words*= num_words_test\n",
    "\n",
    "test_li_tags = ['']\n",
    "test_li_tags*= num_words_test\n",
    "\n",
    "output_li = ['']\n",
    "output_li*= num_words_test\n",
    "\n",
    "output_li_baseline = ['']\n",
    "output_li_baseline*= num_words_test\n",
    "\n",
    "num_errors = 0\n",
    "num_errors_baseline = 0\n",
    "\n",
    "for i in range(num_words_test):\n",
    "    temp_li = te_li[i].split(\"/\")\n",
    "    test_li_words[i] = temp_li[0]\n",
    "    if temp_li[1] in noun_reduced_list:\n",
    "        test_li_tags[i] = 'N'\n",
    "    elif temp_li[1] in verb_reduced_list:\n",
    "        test_li_tags[i] = 'V'\n",
    "    elif temp_li[1] in adjec_reduced_list:\n",
    "        test_li_tags[i] = 'ADJ'\n",
    "    elif temp_li[1] in adv_reduced_list:\n",
    "        test_li_tags[i] = 'ADV'        \n",
    "    elif temp_li[1] in pronoun_reduced_list:\n",
    "        test_li_tags[i] = 'PRO'\n",
    "    else:\n",
    "        test_li_tags[i] = temp_li[1]\n",
    "\n",
    "\n",
    "    output_li_baseline[i] = dict_word_tag_baseline.get(temp_li[0],'')\n",
    "    #If unknown word - tag = 'N'\n",
    "    if output_li_baseline[i]=='':\n",
    "        output_li_baseline[i]='N'\n",
    "        \n",
    "\n",
    "\n",
    "    if output_li_baseline[i]!=test_li_tags[i]:\n",
    "        num_errors_baseline+=1\n",
    "\n",
    "    \n",
    "    if i==0:    #Accounting for the 1st word in the test document for the Viterbi\n",
    "        di_transition_probs = dict2_tag_follow_tag_['.']\n",
    "    else:\n",
    "        di_transition_probs = dict2_tag_follow_tag_[output_li[i-1]]\n",
    "        \n",
    "    di_emission_probs = dict2_word_tag.get(test_li_words[i],'')\n",
    "\n",
    "    #If unknown word  - tag = 'N'\n",
    "    if di_emission_probs=='':\n",
    "        output_li[i]='N'\n",
    "        \n",
    "    else:\n",
    "        max_prod_prob = 0\n",
    "        counter_trans = 0\n",
    "        counter_emis =0\n",
    "        prod_prob = 0\n",
    "        while counter_trans < len(di_transition_probs) and counter_emis < len(di_emission_probs):\n",
    "            tag_tr = di_transition_probs[counter_trans][0]\n",
    "            tag_em = di_emission_probs[counter_emis][0]\n",
    "            if tag_tr < tag_em:\n",
    "                counter_trans+=1\n",
    "            elif tag_tr > tag_em:\n",
    "                counter_emis+=1\n",
    "            else:\n",
    "                prod_prob = di_transition_probs[counter_trans][1] * di_emission_probs[counter_emis][1]\n",
    "                if prod_prob > max_prod_prob:\n",
    "                    max_prod_prob = prod_prob\n",
    "                    output_li[i] = tag_tr\n",
    "                    print (\"i=\",i,\" and output=\",output_li[i])\n",
    "                counter_trans+=1\n",
    "                counter_emis+=1    \n",
    "\n",
    "    if output_li[i]=='': #In case there are no matching entries between the transition tags and emission tags, we choose the most frequent emission tag\n",
    "        output_li[i] = max(di_emission_probs,key=itemgetter(1))[0]  \n",
    "        \n",
    "    if output_li[i]!=test_li_tags[i]:\n",
    "        num_errors+=1\n",
    "\n",
    "                    \n",
    "print (\"Fraction of errors (Baseline) :\",(num_errors_baseline/num_words_test))\n",
    "print (\"Fraction of errors (Viterbi):\",(num_errors/num_words_test))\n",
    "\n",
    "\n",
    "\n",
    "print (\"Tags suggested by Baseline Algorithm:\", output_li_baseline)\n",
    "\n",
    "print (\"Tags suggested by Viterbi Algorithm:\", output_li)\n",
    "\n",
    "print (\"Correct tags:\",test_li_tags)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "364c8985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[')', ',', '.', ':', 'ADJ', 'ADV', 'CC', 'CD', 'DT', 'FW', 'IN', 'MD', 'N', 'POS', 'PRO', 'TO', 'V', 'WDT', 'WP', '``']\n",
      "{'DT': 19, 'N': 17, 'PRO': 3, 'CD': 7, ',': 1, 'ADV': 1, 'ADJ': 3}\n",
      "{'V': 2}\n",
      "i= 1  and output= V\n",
      "i= 2  and output= ADJ\n",
      "Fraction of errors (Baseline) : 0.0\n",
      "Fraction of errors (Viterbi): 0.0\n",
      "Tags suggested by Baseline Algorithm: ['N', 'V', 'ADJ', 'N']\n",
      "Tags suggested by Viterbi Algorithm: ['N', 'V', 'ADJ', 'N']\n",
      "Correct tags: ['N', 'V', 'ADJ', 'N']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division #To avoid integer division\n",
    "from operator import itemgetter\n",
    "###Training Phase###\n",
    "\n",
    "with open(\"wsj_training.txt\", \"r\") as myfile:\n",
    "    tr_str = myfile.read()\n",
    "tr_li = tr_str.split()\n",
    "num_words_train = len(tr_li)\n",
    "\n",
    "\n",
    "train_li_words = ['']\n",
    "train_li_words*= num_words_train\n",
    "\n",
    "train_li_tags = ['']\n",
    "train_li_tags*= num_words_train\n",
    "\n",
    "noun_reduced_list = ['NN','NNS','NNP','NNPS']\n",
    "verb_reduced_list = ['VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "adjec_reduced_list = ['JJ', 'JJR', 'JJS']\n",
    "adv_reduced_list = ['RB', 'RBR', 'RBS']\n",
    "pronoun_reduced_list = ['PRP', 'PRP$', 'RP']\n",
    "\n",
    "for i in range(num_words_train):\n",
    "    temp_li = tr_li[i].split(\"/\")\n",
    "    train_li_words[i] = temp_li[0]\n",
    "    if temp_li[1] in noun_reduced_list:\n",
    "        train_li_tags[i] = 'N'\n",
    "    elif temp_li[1] in verb_reduced_list:\n",
    "        train_li_tags[i] = 'V'\n",
    "    elif temp_li[1] in adjec_reduced_list:\n",
    "        train_li_tags[i] = 'ADJ'\n",
    "    elif temp_li[1] in adv_reduced_list:\n",
    "        train_li_tags[i] = 'ADV'        \n",
    "    elif temp_li[1] in pronoun_reduced_list:\n",
    "        train_li_tags[i] = 'PRO'\n",
    "    else:\n",
    "        train_li_tags[i] = temp_li[1]\n",
    "\n",
    "k = sorted(list(set(train_li_tags)))\n",
    "print (k)\n",
    "dict2_tag_follow_tag_ = {}\n",
    "\"\"\"Nested dictionary to store the transition probabilities\n",
    "each tag A is a key of the outer dictionary\n",
    "the inner dictionary is the corresponding value\n",
    "The inner dictionary's key is the tag B following A\n",
    "and the corresponding value is the number of times B follows A\n",
    "\"\"\"\n",
    "\n",
    "dict2_word_tag = {}\n",
    "\"\"\"Nested dictionary to store the emission probabilities.\n",
    "Each word W is a key of the outer dictionary\n",
    "The inner dictionary is the corresponding value\n",
    "The inner dictionary's key is the tag A of the word W\n",
    "and the corresponding value is the number of times A is a tag of W\n",
    "\"\"\"\n",
    "\n",
    "dict_word_tag_baseline = {}\n",
    "#Dictionary with word as key and its most frequent tag as value\n",
    "\n",
    "for i in range(num_words_train-1):\n",
    "    outer_key = train_li_tags[i]\n",
    "    inner_key = train_li_tags[i+1]\n",
    "    dict2_tag_follow_tag_[outer_key]=dict2_tag_follow_tag_.get(outer_key,{})\n",
    "    dict2_tag_follow_tag_[outer_key][inner_key] = dict2_tag_follow_tag_[outer_key].get(inner_key,0)\n",
    "    dict2_tag_follow_tag_[outer_key][inner_key]+=1\n",
    "\n",
    "    outer_key = train_li_words[i]\n",
    "    inner_key = train_li_tags[i]\n",
    "    dict2_word_tag[outer_key]=dict2_word_tag.get(outer_key,{})\n",
    "    dict2_word_tag[outer_key][inner_key] = dict2_word_tag[outer_key].get(inner_key,0)\n",
    "    dict2_word_tag[outer_key][inner_key]+=1\n",
    "\n",
    "\n",
    "\"\"\"The 1st token is indicated by being the 1st word of a senetence, that is the word after period(.)\n",
    "Adjusting for the fact that the first word of the document is not accounted for that way\n",
    "\"\"\"\n",
    "\n",
    "dict2_tag_follow_tag_['.'] = dict2_tag_follow_tag_.get('.',{})\n",
    "dict2_tag_follow_tag_['.'][train_li_tags[0]] = dict2_tag_follow_tag_['.'].get(train_li_tags[0],0)\n",
    "dict2_tag_follow_tag_['.'][train_li_tags[0]]+=1\n",
    "\n",
    "\n",
    "print (dict2_tag_follow_tag_['IN'])\n",
    "print (dict2_word_tag['made'])\n",
    "\n",
    "last_index = num_words_train-1\n",
    "\n",
    "#Accounting for the last word-tag pair\n",
    "outer_key = train_li_words[last_index]\n",
    "inner_key = train_li_tags[last_index]\n",
    "dict2_word_tag[outer_key]=dict2_word_tag.get(outer_key,{})\n",
    "dict2_word_tag[outer_key][inner_key] = dict2_word_tag[outer_key].get(inner_key,0)\n",
    "dict2_word_tag[outer_key][inner_key]+=1\n",
    "\n",
    "\n",
    "\"\"\"Converting counts to probabilities in the two nested dictionaries\n",
    "& also converting the nested dictionaries to outer dictionary with inner sorted lists\n",
    "\"\"\"\n",
    "for key in dict2_tag_follow_tag_:\n",
    "    di = dict2_tag_follow_tag_[key]\n",
    "    s = sum(di.values())\n",
    "    for innkey in di:\n",
    "        di[innkey] /= s\n",
    "    di = di.items()\n",
    "    di = sorted(di,key=lambda x: x[0])\n",
    "    dict2_tag_follow_tag_[key] = di\n",
    "\n",
    "for key in dict2_word_tag:\n",
    "    di = dict2_word_tag[key]\n",
    "    dict_word_tag_baseline[key] = max(di, key=di.get)\n",
    "    s = sum(di.values())\n",
    "    for innkey in di:\n",
    "        di[innkey] /= s\n",
    "    di = di.items()\n",
    "    di = sorted(di,key=lambda x: x[0])\n",
    "    dict2_word_tag[key] = di\n",
    "\n",
    "\n",
    "\n",
    "###Testing Phase###    \n",
    "\n",
    "with open(\"test.txt\", \"r\") as myfile:\n",
    "    te_str = myfile.read()\n",
    "\n",
    "te_li = te_str.split()\n",
    "num_words_test = len(te_li)\n",
    "\n",
    "test_li_words = ['']\n",
    "test_li_words*= num_words_test\n",
    "\n",
    "test_li_tags = ['']\n",
    "test_li_tags*= num_words_test\n",
    "\n",
    "output_li = ['']\n",
    "output_li*= num_words_test\n",
    "\n",
    "output_li_baseline = ['']\n",
    "output_li_baseline*= num_words_test\n",
    "\n",
    "num_errors = 0\n",
    "num_errors_baseline = 0\n",
    "\n",
    "for i in range(num_words_test):\n",
    "    temp_li = te_li[i].split(\"/\")\n",
    "    test_li_words[i] = temp_li[0]\n",
    "    if temp_li[1] in noun_reduced_list:\n",
    "        test_li_tags[i] = 'N'\n",
    "    elif temp_li[1] in verb_reduced_list:\n",
    "        test_li_tags[i] = 'V'\n",
    "    elif temp_li[1] in adjec_reduced_list:\n",
    "        test_li_tags[i] = 'ADJ'\n",
    "    elif temp_li[1] in adv_reduced_list:\n",
    "        test_li_tags[i] = 'ADV'        \n",
    "    elif temp_li[1] in pronoun_reduced_list:\n",
    "        test_li_tags[i] = 'PRO'\n",
    "    else:\n",
    "        test_li_tags[i] = temp_li[1]\n",
    "\n",
    "\n",
    "    output_li_baseline[i] = dict_word_tag_baseline.get(temp_li[0],'')\n",
    "    #If unknown word - tag = 'N'\n",
    "    if output_li_baseline[i]=='':\n",
    "        output_li_baseline[i]='N'\n",
    "        \n",
    "\n",
    "\n",
    "    if output_li_baseline[i]!=test_li_tags[i]:\n",
    "        num_errors_baseline+=1\n",
    "\n",
    "    \n",
    "    if i==0:    #Accounting for the 1st word in the test document for the Viterbi\n",
    "        di_transition_probs = dict2_tag_follow_tag_['.']\n",
    "    else:\n",
    "        di_transition_probs = dict2_tag_follow_tag_[output_li[i-1]]\n",
    "        \n",
    "    di_emission_probs = dict2_word_tag.get(test_li_words[i],'')\n",
    "\n",
    "    #If unknown word  - tag = 'N'\n",
    "    if di_emission_probs=='':\n",
    "        output_li[i]='N'\n",
    "        \n",
    "    else:\n",
    "        max_prod_prob = 0\n",
    "        counter_trans = 0\n",
    "        counter_emis =0\n",
    "        prod_prob = 0\n",
    "        while counter_trans < len(di_transition_probs) and counter_emis < len(di_emission_probs):\n",
    "            tag_tr = di_transition_probs[counter_trans][0]\n",
    "            tag_em = di_emission_probs[counter_emis][0]\n",
    "            if tag_tr < tag_em:\n",
    "                counter_trans+=1\n",
    "            elif tag_tr > tag_em:\n",
    "                counter_emis+=1\n",
    "            else:\n",
    "                prod_prob = di_transition_probs[counter_trans][1] * di_emission_probs[counter_emis][1]\n",
    "                if prod_prob > max_prod_prob:\n",
    "                    max_prod_prob = prod_prob\n",
    "                    output_li[i] = tag_tr\n",
    "                    print (\"i=\",i,\" and output=\",output_li[i])\n",
    "                counter_trans+=1\n",
    "                counter_emis+=1    \n",
    "\n",
    "    if output_li[i]=='': #In case there are no matching entries between the transition tags and emission tags, we choose the most frequent emission tag\n",
    "        output_li[i] = max(di_emission_probs,key=itemgetter(1))[0]  \n",
    "        \n",
    "    if output_li[i]!=test_li_tags[i]:\n",
    "        num_errors+=1\n",
    "\n",
    "                    \n",
    "print (\"Fraction of errors (Baseline) :\",(num_errors_baseline/num_words_test))\n",
    "print (\"Fraction of errors (Viterbi):\",(num_errors/num_words_test))\n",
    "\n",
    "\n",
    "\n",
    "print (\"Tags suggested by Baseline Algorithm:\", output_li_baseline)\n",
    "\n",
    "print (\"Tags suggested by Viterbi Algorithm:\", output_li)\n",
    "\n",
    "print (\"Correct tags:\",test_li_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8001de51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>word</th>\n",
       "      <th>pos_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>aa</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>aaa</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>aah</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>aahed</td>\n",
       "      <td>VBN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>aahing</td>\n",
       "      <td>VBG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    word pos_tag\n",
       "0           0      aa      NN\n",
       "1           1     aaa      NN\n",
       "2           2     aah      NN\n",
       "3           3   aahed     VBN\n",
       "4           4  aahing     VBG"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df =  pd.read_csv(r\"C:\\Users\\raval\\jupyter_notebook\\NLP\\words_pos.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a8b320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"Unnamed: 0\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92871ca1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple expected at most 1 argument, got 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mword\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpos_tag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: tuple expected at most 1 argument, got 2"
     ]
    }
   ],
   "source": [
    "tuple(df[\"word\"],df[\"pos_tag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b211aa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"wsj_training.txt\", \"r\") as myfile:\n",
    "#     tr_str = myfile.read()\n",
    "# tr_li = tr_str.split()\n",
    "# num_words_train = len(tr_li)\n",
    "\n",
    "tr_li = tuple(df[\"word\"],df[\"pos_tag\"])\n",
    "\n",
    "num_words_train=len(df)\n",
    "\n",
    "train_li_words = ['']\n",
    "train_li_words*= num_words_train\n",
    "\n",
    "train_li_tags = ['']\n",
    "train_li_tags*= num_words_train\n",
    "\n",
    "noun_reduced_list = ['NN','NNS','NNP','NNPS']\n",
    "verb_reduced_list = ['VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "adjec_reduced_list = ['JJ', 'JJR', 'JJS']\n",
    "adv_reduced_list = ['RB', 'RBR', 'RBS']\n",
    "pronoun_reduced_list = ['PRP', 'PRP$', 'RP']\n",
    "\n",
    "for i in range(num_words_train):\n",
    "    temp_li = df[\"word\"]\n",
    "#     temp_li = tr_li[i].split(\"/\")\n",
    "    train_li_words[i] = temp_li[0]\n",
    "    if temp_li[1] in noun_reduced_list:\n",
    "        train_li_tags[i] = 'N'\n",
    "    elif temp_li[1] in verb_reduced_list:\n",
    "        train_li_tags[i] = 'V'\n",
    "    elif temp_li[1] in adjec_reduced_list:\n",
    "        train_li_tags[i] = 'ADJ'\n",
    "    elif temp_li[1] in adv_reduced_list:\n",
    "        train_li_tags[i] = 'ADV'        \n",
    "    elif temp_li[1] in pronoun_reduced_list:\n",
    "        train_li_tags[i] = 'PRO'\n",
    "    else:\n",
    "        train_li_tags[i] = temp_li[1]\n",
    "\n",
    "k = sorted(list(set(train_li_tags)))\n",
    "print (k)\n",
    "dict2_tag_follow_tag_ = {}\n",
    "\"\"\"Nested dictionary to store the transition probabilities\n",
    "each tag A is a key of the outer dictionary\n",
    "the inner dictionary is the corresponding value\n",
    "The inner dictionary's key is the tag B following A\n",
    "and the corresponding value is the number of times B follows A\n",
    "\"\"\"\n",
    "\n",
    "dict2_word_tag = {}\n",
    "\"\"\"Nested dictionary to store the emission probabilities.\n",
    "Each word W is a key of the outer dictionary\n",
    "The inner dictionary is the corresponding value\n",
    "The inner dictionary's key is the tag A of the word W\n",
    "and the corresponding value is the number of times A is a tag of W\n",
    "\"\"\"\n",
    "\n",
    "dict_word_tag_baseline = {}\n",
    "#Dictionary with word as key and its most frequent tag as value\n",
    "\n",
    "for i in range(num_words_train-1):\n",
    "    outer_key = train_li_tags[i]\n",
    "    inner_key = train_li_tags[i+1]\n",
    "    dict2_tag_follow_tag_[outer_key]=dict2_tag_follow_tag_.get(outer_key,{})\n",
    "    dict2_tag_follow_tag_[outer_key][inner_key] = dict2_tag_follow_tag_[outer_key].get(inner_key,0)\n",
    "    dict2_tag_follow_tag_[outer_key][inner_key]+=1\n",
    "\n",
    "    outer_key = train_li_words[i]\n",
    "    inner_key = train_li_tags[i]\n",
    "    dict2_word_tag[outer_key]=dict2_word_tag.get(outer_key,{})\n",
    "    dict2_word_tag[outer_key][inner_key] = dict2_word_tag[outer_key].get(inner_key,0)\n",
    "    dict2_word_tag[outer_key][inner_key]+=1\n",
    "\n",
    "\n",
    "\"\"\"The 1st token is indicated by being the 1st word of a senetence, that is the word after period(.)\n",
    "Adjusting for the fact that the first word of the document is not accounted for that way\n",
    "\"\"\"\n",
    "\n",
    "dict2_tag_follow_tag_['.'] = dict2_tag_follow_tag_.get('.',{})\n",
    "dict2_tag_follow_tag_['.'][train_li_tags[0]] = dict2_tag_follow_tag_['.'].get(train_li_tags[0],0)\n",
    "dict2_tag_follow_tag_['.'][train_li_tags[0]]+=1\n",
    "\n",
    "\n",
    "print (dict2_tag_follow_tag_['IN'])\n",
    "print (dict2_word_tag['made'])\n",
    "\n",
    "last_index = num_words_train-1\n",
    "\n",
    "#Accounting for the last word-tag pair\n",
    "outer_key = train_li_words[last_index]\n",
    "inner_key = train_li_tags[last_index]\n",
    "dict2_word_tag[outer_key]=dict2_word_tag.get(outer_key,{})\n",
    "dict2_word_tag[outer_key][inner_key] = dict2_word_tag[outer_key].get(inner_key,0)\n",
    "dict2_word_tag[outer_key][inner_key]+=1\n",
    "\n",
    "\n",
    "\"\"\"Converting counts to probabilities in the two nested dictionaries\n",
    "& also converting the nested dictionaries to outer dictionary with inner sorted lists\n",
    "\"\"\"\n",
    "for key in dict2_tag_follow_tag_:\n",
    "    di = dict2_tag_follow_tag_[key]\n",
    "    s = sum(di.values())\n",
    "    for innkey in di:\n",
    "        di[innkey] /= s\n",
    "    di = di.items()\n",
    "    di = sorted(di,key=lambda x: x[0])\n",
    "    dict2_tag_follow_tag_[key] = di\n",
    "\n",
    "for key in dict2_word_tag:\n",
    "    di = dict2_word_tag[key]\n",
    "    dict_word_tag_baseline[key] = max(di, key=di.get)\n",
    "    s = sum(di.values())\n",
    "    for innkey in di:\n",
    "        di[innkey] /= s\n",
    "    di = di.items()\n",
    "    di = sorted(di,key=lambda x: x[0])\n",
    "    dict2_word_tag[key] = di\n",
    "\n",
    "\n",
    "\n",
    "###Testing Phase###    \n",
    "\n",
    "with open(\"test.txt\", \"r\") as myfile:\n",
    "    te_str = myfile.read()\n",
    "\n",
    "te_li = te_str.split()\n",
    "num_words_test = len(te_li)\n",
    "\n",
    "test_li_words = ['']\n",
    "test_li_words*= num_words_test\n",
    "\n",
    "test_li_tags = ['']\n",
    "test_li_tags*= num_words_test\n",
    "\n",
    "output_li = ['']\n",
    "output_li*= num_words_test\n",
    "\n",
    "output_li_baseline = ['']\n",
    "output_li_baseline*= num_words_test\n",
    "\n",
    "num_errors = 0\n",
    "num_errors_baseline = 0\n",
    "\n",
    "for i in range(num_words_test):\n",
    "    temp_li = te_li[i].split(\"/\")\n",
    "    test_li_words[i] = temp_li[0]\n",
    "    if temp_li[1] in noun_reduced_list:\n",
    "        test_li_tags[i] = 'N'\n",
    "    elif temp_li[1] in verb_reduced_list:\n",
    "        test_li_tags[i] = 'V'\n",
    "    elif temp_li[1] in adjec_reduced_list:\n",
    "        test_li_tags[i] = 'ADJ'\n",
    "    elif temp_li[1] in adv_reduced_list:\n",
    "        test_li_tags[i] = 'ADV'        \n",
    "    elif temp_li[1] in pronoun_reduced_list:\n",
    "        test_li_tags[i] = 'PRO'\n",
    "    else:\n",
    "        test_li_tags[i] = temp_li[1]\n",
    "\n",
    "\n",
    "    output_li_baseline[i] = dict_word_tag_baseline.get(temp_li[0],'')\n",
    "    #If unknown word - tag = 'N'\n",
    "    if output_li_baseline[i]=='':\n",
    "        output_li_baseline[i]='N'\n",
    "        \n",
    "\n",
    "\n",
    "    if output_li_baseline[i]!=test_li_tags[i]:\n",
    "        num_errors_baseline+=1\n",
    "\n",
    "    \n",
    "    if i==0:    #Accounting for the 1st word in the test document for the Viterbi\n",
    "        di_transition_probs = dict2_tag_follow_tag_['.']\n",
    "    else:\n",
    "        di_transition_probs = dict2_tag_follow_tag_[output_li[i-1]]\n",
    "        \n",
    "    di_emission_probs = dict2_word_tag.get(test_li_words[i],'')\n",
    "\n",
    "    #If unknown word  - tag = 'N'\n",
    "    if di_emission_probs=='':\n",
    "        output_li[i]='N'\n",
    "        \n",
    "    else:\n",
    "        max_prod_prob = 0\n",
    "        counter_trans = 0\n",
    "        counter_emis =0\n",
    "        prod_prob = 0\n",
    "        while counter_trans < len(di_transition_probs) and counter_emis < len(di_emission_probs):\n",
    "            tag_tr = di_transition_probs[counter_trans][0]\n",
    "            tag_em = di_emission_probs[counter_emis][0]\n",
    "            if tag_tr < tag_em:\n",
    "                counter_trans+=1\n",
    "            elif tag_tr > tag_em:\n",
    "                counter_emis+=1\n",
    "            else:\n",
    "                prod_prob = di_transition_probs[counter_trans][1] * di_emission_probs[counter_emis][1]\n",
    "                if prod_prob > max_prod_prob:\n",
    "                    max_prod_prob = prod_prob\n",
    "                    output_li[i] = tag_tr\n",
    "                    print (\"i=\",i,\" and output=\",output_li[i])\n",
    "                counter_trans+=1\n",
    "                counter_emis+=1    \n",
    "\n",
    "    if output_li[i]=='': #In case there are no matching entries between the transition tags and emission tags, we choose the most frequent emission tag\n",
    "        output_li[i] = max(di_emission_probs,key=itemgetter(1))[0]  \n",
    "        \n",
    "    if output_li[i]!=test_li_tags[i]:\n",
    "        num_errors+=1\n",
    "\n",
    "                    \n",
    "print (\"Fraction of errors (Baseline) :\",(num_errors_baseline/num_words_test))\n",
    "print (\"Fraction of errors (Viterbi):\",(num_errors/num_words_test))\n",
    "\n",
    "\n",
    "\n",
    "print (\"Tags suggested by Baseline Algorithm:\", output_li_baseline)\n",
    "\n",
    "print (\"Tags suggested by Viterbi Algorithm:\", output_li)\n",
    "\n",
    "print (\"Correct tags:\",test_li_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc941a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/37011\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m temp_li \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m][i]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m test_li_words[i] \u001b[38;5;241m=\u001b[39m temp_li[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtemp_li\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;129;01min\u001b[39;00m noun_reduced_list:\n\u001b[0;32m     24\u001b[0m     test_li_tags[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m temp_li[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m verb_reduced_list:\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "###Testing Phase###    \n",
    "\n",
    "with open(\"test.txt\", \"r\") as myfile:\n",
    "    te_str = myfile.read()\n",
    "\n",
    "te_li = te_str.split()\n",
    "num_words_test = len(te_li)\n",
    "\n",
    "test_li_words = ['']\n",
    "test_li_words*= num_words_test\n",
    "\n",
    "test_li_tags = ['']\n",
    "test_li_tags*= num_words_test\n",
    "\n",
    "output_li = ['']\n",
    "output_li*= num_words_test\n",
    "\n",
    "output_li_baseline = ['']\n",
    "output_li_baseline*= num_words_test\n",
    "\n",
    "num_errors = 0\n",
    "num_errors_baseline = 0\n",
    "\n",
    "for i in range(num_words_test):\n",
    "    temp_li = te_li[i].split(\"/\")\n",
    "    test_li_words[i] = temp_li[0]\n",
    "    if temp_li[1] in noun_reduced_list:\n",
    "        test_li_tags[i] = 'N'\n",
    "    elif temp_li[1] in verb_reduced_list:\n",
    "        test_li_tags[i] = 'V'\n",
    "    elif temp_li[1] in adjec_reduced_list:\n",
    "        test_li_tags[i] = 'ADJ'\n",
    "    elif temp_li[1] in adv_reduced_list:\n",
    "        test_li_tags[i] = 'ADV'        \n",
    "    elif temp_li[1] in pronoun_reduced_list:\n",
    "        test_li_tags[i] = 'PRO'\n",
    "    else:\n",
    "        test_li_tags[i] = temp_li[1]\n",
    "\n",
    "\n",
    "    output_li_baseline[i] = dict_word_tag_baseline.get(temp_li[0],'')\n",
    "    #If unknown word - tag = 'N'\n",
    "    if output_li_baseline[i]=='':\n",
    "        output_li_baseline[i]='N'\n",
    "        \n",
    "\n",
    "\n",
    "    if output_li_baseline[i]!=test_li_tags[i]:\n",
    "        num_errors_baseline+=1\n",
    "\n",
    "    \n",
    "    if i==0:    #Accounting for the 1st word in the test document for the Viterbi\n",
    "        di_transition_probs = dict2_tag_follow_tag_['.']\n",
    "    else:\n",
    "        di_transition_probs = dict2_tag_follow_tag_[output_li[i-1]]\n",
    "        \n",
    "    di_emission_probs = dict2_word_tag.get(test_li_words[i],'')\n",
    "\n",
    "    #If unknown word  - tag = 'N'\n",
    "    if di_emission_probs=='':\n",
    "        output_li[i]='N'\n",
    "        \n",
    "    else:\n",
    "        max_prod_prob = 0\n",
    "        counter_trans = 0\n",
    "        counter_emis =0\n",
    "        prod_prob = 0\n",
    "        while counter_trans < len(di_transition_probs) and counter_emis < len(di_emission_probs):\n",
    "            tag_tr = di_transition_probs[counter_trans][0]\n",
    "            tag_em = di_emission_probs[counter_emis][0]\n",
    "            if tag_tr < tag_em:\n",
    "                counter_trans+=1\n",
    "            elif tag_tr > tag_em:\n",
    "                counter_emis+=1\n",
    "            else:\n",
    "                prod_prob = di_transition_probs[counter_trans][1] * di_emission_probs[counter_emis][1]\n",
    "                if prod_prob > max_prod_prob:\n",
    "                    max_prod_prob = prod_prob\n",
    "                    output_li[i] = tag_tr\n",
    "                    print (\"i=\",i,\" and output=\",output_li[i])\n",
    "                counter_trans+=1\n",
    "                counter_emis+=1    \n",
    "\n",
    "    if output_li[i]=='': #In case there are no matching entries between the transition tags and emission tags, we choose the most frequent emission tag\n",
    "        output_li[i] = max(di_emission_probs,key=itemgetter(1))[0]  \n",
    "        \n",
    "    if output_li[i]!=test_li_tags[i]:\n",
    "        num_errors+=1\n",
    "\n",
    "                    \n",
    "print (\"Fraction of errors (Baseline) :\",(num_errors_baseline/num_words_test))\n",
    "print (\"Fraction of errors (Viterbi):\",(num_errors/num_words_test))\n",
    "\n",
    "\n",
    "\n",
    "print (\"Tags suggested by Baseline Algorithm:\", output_li_baseline)\n",
    "\n",
    "print (\"Tags suggested by Viterbi Algorithm:\", output_li)\n",
    "\n",
    "print (\"Correct tags:\",test_li_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a2608c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
