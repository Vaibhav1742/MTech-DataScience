{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYxk4HBO_bkP"
   },
   "source": [
    "# Assignment 17\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_Mfq9mS-V9z"
   },
   "source": [
    "Implement following CUDA programs\n",
    "1.   to print hello message on the screen using kernal function\n",
    "2.   to add two vectors of size 100 and 20000 abd analyze the performance comparison between cpu and gpu processing\n",
    "3. to multply two matrix of size 20 X 20 and 1024 X 1024 analyze the performance comparison between cpu and gpu processing\n",
    "4. to obtain CUDA device information and print the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F244pwPVBOP0",
    "outputId": "6f99d009-a5f4-4c1a-d9b9-c1fed1dd7cca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycuda\n",
      "  Downloading pycuda-2024.1.tar.gz (1.7 MB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.7 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting pytools>=2011.2 (from pycuda)\n",
      "  Downloading pytools-2024.1.1-py2.py3-none-any.whl (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.1/85.1 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from pycuda) (1.4.4)\n",
      "Collecting mako (from pycuda)\n",
      "  Downloading Mako-1.3.3-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (4.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (4.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from mako->pycuda) (2.1.5)\n",
      "Building wheels for collected packages: pycuda\n",
      "  Building wheel for pycuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pycuda: filename=pycuda-2024.1-cp310-cp310-linux_x86_64.whl size=661204 sha256=c6d745905ea408f8a73a79e4656b609ac0f9dced62157e4766697031d2a03d17\n",
      "  Stored in directory: /root/.cache/pip/wheels/12/34/d2/9a349255a4eca3a486d82c79d21e138ce2ccd90f414d9d72b8\n",
      "Successfully built pycuda\n",
      "Installing collected packages: pytools, mako, pycuda\n",
      "Successfully installed mako-1.3.3 pycuda-2024.1 pytools-2024.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pycuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyHhrpb1C8Jd"
   },
   "source": [
    "## 1. To print hello message on the screen using kernal function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8wjiOh49CoXL",
    "outputId": "2ed570d1-36c9-4167-9ac6-159e37bcf28c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hello_1_1.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile hello_1_1.cu\n",
    "\n",
    "#include <stdio.h>\n",
    "\n",
    "__global__ void cuda_hello_1_1() {\n",
    "    printf(\"Hello World from GPU with grid dimension (1, 1) and block dimension (1, 1)!\\n\");\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    cuda_hello_1_1<<<1,1>>>();\n",
    "    cudaDeviceSynchronize(); // Make sure all GPU work is done before exiting\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "zPty75TRCtXw"
   },
   "outputs": [],
   "source": [
    "!nvcc -o hello_1_1 hello_1_1.cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FZwhczAQCvae",
    "outputId": "dbb6cee2-bef1-48ec-df14-98454d3b226a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World from GPU with grid dimension (1, 1) and block dimension (1, 1)!\n"
     ]
    }
   ],
   "source": [
    "!./hello_1_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFAlJGUtQRrZ"
   },
   "source": [
    "## 2. To add two vectors of size 100 and 20000 and analyze the performance comparison between cpu and gpu processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0DteexaDL5l"
   },
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ic9n6ESmDQ6y"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rYHKYZ7jDVP8",
    "outputId": "90edcd1c-323a-43d3-8408-cc3ea81a9693"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector addition of size 100 on GPU took 0.0006387233734130859 seconds.\n",
      "Vector addition of size 20000 on GPU took 5.435943603515625e-05 seconds.\n"
     ]
    }
   ],
   "source": [
    "cuda_kernel_code = \"\"\"\n",
    "__global__ void vector_add(float *a, float *b, float *c, int n) {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < n) {\n",
    "        c[i] = a[i] + b[i];\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "cuda_module = SourceModule(cuda_kernel_code)\n",
    "\n",
    "vector_add_cuda = cuda_module.get_function(\"vector_add\")\n",
    "\n",
    "\n",
    "def vector_add_gpu(a, b):\n",
    "    n = a.size\n",
    "\n",
    "    a_gpu = cuda.mem_alloc(a.nbytes)\n",
    "    b_gpu = cuda.mem_alloc(b.nbytes)\n",
    "    c_gpu = cuda.mem_alloc(b.nbytes)\n",
    "\n",
    "    cuda.memcpy_htod(a_gpu, a)\n",
    "    cuda.memcpy_htod(b_gpu, b)\n",
    "\n",
    "    block_dim = (256, 1, 1)\n",
    "    grid_dim = ((n + block_dim[0] - 1) // block_dim[0], 1)\n",
    "\n",
    "    start_time = time.time()\n",
    "    vector_add_cuda(a_gpu, b_gpu, c_gpu, np.int32(n), block=block_dim, grid=grid_dim)\n",
    "\n",
    "    cuda.Context.synchronize()\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    c = np.empty_like(a)\n",
    "    cuda.memcpy_dtoh(c, c_gpu)\n",
    "\n",
    "    return c, end_time - start_time\n",
    "\n",
    "vector_size_1 = 100\n",
    "vector_size_2 = 20000\n",
    "a = np.random.randn(vector_size_2).astype(np.float32)\n",
    "b = np.random.randn(vector_size_2).astype(np.float32)\n",
    "\n",
    "result_gpu1, gpu_time1 = vector_add_gpu(a[:vector_size_1], b[:vector_size_1])\n",
    "result_gpu2, gpu_time2 = vector_add_gpu(a[:vector_size_2], b[:vector_size_2])\n",
    "\n",
    "print(\"Vector addition of size\", vector_size_1, \"on GPU took\", gpu_time1, \"seconds.\")\n",
    "print(\"Vector addition of size\", vector_size_2, \"on GPU took\", gpu_time2, \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AyWLpU9gD0mU"
   },
   "source": [
    "### CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "U0vYepL-D1bG"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qbXkau8jD22d"
   },
   "outputs": [],
   "source": [
    "def vector_add_cpu(a, b):\n",
    "    start_time = time.time()\n",
    "    result = a + b\n",
    "    end_time = time.time()\n",
    "    return result, end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zzXaKqiHD3vD",
    "outputId": "15bfe82b-1464-427c-c1d9-9177ada816a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector addition of size 100 on CPU took 2.384185791015625e-05 seconds.\n",
      "Vector addition of size 20000 on CPU took 2.0742416381835938e-05 seconds.\n"
     ]
    }
   ],
   "source": [
    "vector_size_1 = 100\n",
    "vector_size_2 = 20000\n",
    "a = np.random.randn(vector_size_2).astype(np.float32)\n",
    "b = np.random.randn(vector_size_2).astype(np.float32)\n",
    "\n",
    "result_cpu1, cpu_time1 = vector_add_cpu(a[:vector_size_1], b[:vector_size_1])\n",
    "result_cpu2, cpu_time2 = vector_add_cpu(a[:vector_size_2], b[:vector_size_2])\n",
    "print(\"Vector addition of size\", vector_size_1, \"on CPU took\", cpu_time1, \"seconds.\")\n",
    "print(\"Vector addition of size\", vector_size_2, \"on CPU took\", cpu_time2, \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8UGLhpMOut5"
   },
   "source": [
    "## 3. To multply two matrix of size 20 X 20 and 1024 X 1024 analyze the performance comparison between cpu and gpu processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lyAqm9DEF9d"
   },
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wuAwvgYPMr5T",
    "outputId": "0adaf85f-97b4-4b6b-fa25-3da01ac19eaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matrix size: (20, 20)\n",
      "GPU time: 0.437754 seconds\n",
      "\n",
      "Matrix size: (1024, 1024)\n",
      "GPU time: 0.013317 seconds\n"
     ]
    }
   ],
   "source": [
    "def matrix_multiply_gpu(a, b):\n",
    "    cuda_code = \"\"\"\n",
    "    __global__ void matrix_multiply(float *a, float *b, float *c, int n) {\n",
    "        int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "        int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "        if (row < n && col < n) {\n",
    "            float sum = 0.0;\n",
    "            for (int i = 0; i < n; ++i) {\n",
    "                sum += a[row * n + i] * b[i * n + col];\n",
    "            }\n",
    "            c[row * n + col] = sum;\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    mod = SourceModule(cuda_code)\n",
    "\n",
    "    matrix_multiply_cuda = mod.get_function(\"matrix_multiply\")\n",
    "\n",
    "    a_gpu = cuda.mem_alloc(a.nbytes)\n",
    "    b_gpu = cuda.mem_alloc(b.nbytes)\n",
    "    c_gpu = cuda.mem_alloc(a.nbytes)\n",
    "\n",
    "    cuda.memcpy_htod(a_gpu, a)\n",
    "    cuda.memcpy_htod(b_gpu, b)\n",
    "\n",
    "    block_size = (16, 16, 1)\n",
    "    grid_size = ((a.shape[1] + block_size[0] - 1) // block_size[0], (a.shape[0] + block_size[1] - 1) // block_size[1], 1)\n",
    "\n",
    "    matrix_multiply_cuda(a_gpu, b_gpu, c_gpu, np.int32(a.shape[0]), block=block_size, grid=grid_size)\n",
    "\n",
    "    c = np.empty_like(a)\n",
    "    cuda.memcpy_dtoh(c, c_gpu)\n",
    "\n",
    "    return c\n",
    "\n",
    "def generate_random_matrix(rows, cols):\n",
    "    return np.random.rand(rows, cols).astype(np.float32)\n",
    "\n",
    "def measure_time(matrix_size, func, *args):\n",
    "    start_time = time.time()\n",
    "    result = func(*args)\n",
    "    end_time = time.time()\n",
    "    return result, end_time - start_time\n",
    "\n",
    "matrix_sizes = [(20, 20), (1024, 1024)]\n",
    "\n",
    "for size in matrix_sizes:\n",
    "    print(f\"\\nMatrix size: {size}\")\n",
    "    a = generate_random_matrix(*size)\n",
    "    b = generate_random_matrix(*size)\n",
    "\n",
    "    gpu_result, gpu_time = measure_time(size, matrix_multiply_gpu, a, b)\n",
    "    print(f\"GPU time: {gpu_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ip6tfTOnETmc"
   },
   "source": [
    "### CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dO939cYwEWr0",
    "outputId": "01fb25c4-095f-4801-f78b-662a237b0bf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matrix size: (20, 20)\n",
      "CPU time: 0.009491 seconds\n",
      "\n",
      "Matrix size: (1024, 1024)\n",
      "CPU time: 718.713489 seconds\n"
     ]
    }
   ],
   "source": [
    "def matrix_multiply_cpu(a, b):\n",
    "    result = np.zeros((a.shape[0], b.shape[1]), dtype=np.float32)\n",
    "    for i in range(a.shape[0]):\n",
    "        for j in range(b.shape[1]):\n",
    "            for k in range(a.shape[1]):\n",
    "                result[i, j] += a[i, k] * b[k, j]\n",
    "    return result\n",
    "\n",
    "def generate_random_matrix(rows, cols):\n",
    "    return np.random.rand(rows, cols).astype(np.float32)\n",
    "\n",
    "def measure_time(matrix_size, func, *args):\n",
    "    start_time = time.time()\n",
    "    result = func(*args)\n",
    "    end_time = time.time()\n",
    "    return result, end_time - start_time\n",
    "\n",
    "matrix_sizes = [(20, 20), (1024, 1024)]\n",
    "\n",
    "for size in matrix_sizes:\n",
    "    print(f\"\\nMatrix size: {size}\")\n",
    "    a = generate_random_matrix(*size)\n",
    "    b = generate_random_matrix(*size)\n",
    "\n",
    "    # CPU matrix multiplication\n",
    "    cpu_result, cpu_time = measure_time(size, matrix_multiply_cpu, a, b)\n",
    "    print(f\"CPU time: {cpu_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLnEOawMQZe3"
   },
   "source": [
    "# 4. To obtain CUDA device information and print the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "saAUJ-sDQbJi",
    "outputId": "8d4caa25-f6f3-453f-c272-8d53737f6ac6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CUDA devices: 1\n",
      "\n",
      "CUDA Device: 0\n",
      "  Name: Tesla T4\n",
      "  Compute Capability: (7, 5)\n",
      "  Total Memory: 14.74810791015625 GB\n",
      "  Max Threads per Block: 1024\n",
      "  Multiprocessor Count: 40\n",
      "  Clock Rate: 1.59 GHz\n"
     ]
    }
   ],
   "source": [
    "import pycuda.driver as cuda\n",
    "\n",
    "cuda.init()\n",
    "\n",
    "num_devices = cuda.Device.count()\n",
    "\n",
    "print(\"Number of CUDA devices:\", num_devices)\n",
    "\n",
    "for i in range(num_devices):\n",
    "    device = cuda.Device(i)\n",
    "    print(\"\\nCUDA Device:\", i)\n",
    "    print(\"  Name:\", device.name())\n",
    "    print(\"  Compute Capability:\", device.compute_capability())\n",
    "    print(\"  Total Memory:\", device.total_memory() / (1024 ** 3), \"GB\")\n",
    "    print(\"  Max Threads per Block:\", device.max_threads_per_block)\n",
    "    print(\"  Multiprocessor Count:\", device.multiprocessor_count)\n",
    "    print(\"  Clock Rate:\", device.clock_rate / 1e6, \"GHz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WL8zLKC1EvK6",
    "outputId": "fedfa61b-41c5-42da-b107-eeac5c5f57a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 16 08:54:22 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   58C    P0              28W /  70W |    103MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
