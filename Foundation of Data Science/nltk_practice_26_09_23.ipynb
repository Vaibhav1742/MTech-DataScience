{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb340ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\raval/nltk_data'\n    - 'C:\\\\python311\\\\nltk_data'\n    - 'C:\\\\python311\\\\share\\\\nltk_data'\n    - 'C:\\\\python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\raval\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mC:\\python311\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\python311\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\raval/nltk_data'\n    - 'C:\\\\python311\\\\nltk_data'\n    - 'C:\\\\python311\\\\share\\\\nltk_data'\n    - 'C:\\\\python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\raval\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     35\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour long text here...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 36\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43mextractive_summarization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_sentences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(summary)\n",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m, in \u001b[0;36mextractive_summarization\u001b[1;34m(text, num_sentences)\u001b[0m\n\u001b[0;32m     11\u001b[0m words \u001b[38;5;241m=\u001b[39m word_tokenize(text)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Remove stopwords\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     15\u001b[0m words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Calculate word frequency\u001b[39;00m\n",
      "File \u001b[1;32mC:\\python311\\Lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32mC:\\python311\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32mC:\\python311\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\python311\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\raval/nltk_data'\n    - 'C:\\\\python311\\\\nltk_data'\n    - 'C:\\\\python311\\\\share\\\\nltk_data'\n    - 'C:\\\\python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\raval\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def extractive_summarization(text, num_sentences):\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "    # Calculate word frequency\n",
    "    word_freq = FreqDist(words)\n",
    "\n",
    "    # Calculate sentence scores based on word frequency\n",
    "    sentence_scores = {}\n",
    "    for sentence in sentences:\n",
    "        for word in word_tokenize(sentence.lower()):\n",
    "            if word in word_freq:\n",
    "                if sentence not in sentence_scores:\n",
    "                    sentence_scores[sentence] = word_freq[word]\n",
    "                else:\n",
    "                    sentence_scores[sentence] += word_freq[word]\n",
    "\n",
    "    # Select the top-ranked sentences\n",
    "    summary_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:num_sentences]\n",
    "    return ' '.join(summary_sentences)\n",
    "\n",
    "# Example usage\n",
    "text = \"Your long text here...\"\n",
    "summary = extractive_summarization(text, num_sentences=3)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06f986e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = '''The Machine Learning Tsunami\n",
    "In 2006, Geoffrey Hinton et al. published a paper1\n",
    "showing how to train a deep neural\n",
    "network capable of recognizing handwritten digits with state-of-the-art precision\n",
    "(>98%). They branded this technique “Deep Learning.” Training a deep neural net\n",
    "was widely considered impossible at the time,2\n",
    " and most researchers had abandoned\n",
    "the idea since the 1990s. This paper revived the interest of the scientific community\n",
    "and before long many new papers demonstrated that Deep Learning was not only\n",
    "possible, but capable of mind-blowing achievements that no other Machine Learning\n",
    "(ML) technique could hope to match (with the help of tremendous computing power\n",
    "and great amounts of data). This enthusiasm soon extended to many other areas of\n",
    "Machine Learning.\n",
    "Fast-forward 10 years and Machine Learning has conquered the industry: it is now at\n",
    "the heart of much of the magic in today’s high-tech products, ranking your web\n",
    "search results, powering your smartphone’s speech recognition, recommending vid‐\n",
    "eos, and beating the world champion at the game of Go. Before you know it, it will be\n",
    "driving your car.\n",
    "Machine Learning in Your Projects\n",
    "So naturally you are excited about Machine Learning and you would love to join the\n",
    "party!\n",
    "Perhaps you would like to give your homemade robot a brain of its own? Make it rec‐\n",
    "ognize faces? Or learn to walk around?\n",
    "xi\n",
    "Or maybe your company has tons of data (user logs, financial data, production data,\n",
    "machine sensor data, hotline stats, HR reports, etc.), and more than likely you could\n",
    "unearth some hidden gems if you just knew where to look; for example:\n",
    "• Segment customers and find the best marketing strategy for each group\n",
    "• Recommend products for each client based on what similar clients bought\n",
    "• Detect which transactions are likely to be fraudulent\n",
    "• Forecast next year’s revenue\n",
    "• And more\n",
    "Whatever the reason, you have decided to learn Machine Learning and implement it\n",
    "in your projects. Great idea!\n",
    "Objective and Approach\n",
    "This book assumes that you know close to nothing about Machine Learning. Its goal\n",
    "is to give you the concepts, the intuitions, and the tools you need to actually imple‐\n",
    "ment programs capable of learning from data.\n",
    "We will cover a large number of techniques, from the simplest and most commonly\n",
    "used (such as linear regression) to some of the Deep Learning techniques that regu‐\n",
    "larly win competitions.\n",
    "Rather than implementing our own toy versions of each algorithm, we will be using\n",
    "actual production-ready Python frameworks:\n",
    "• Scikit-Learn is very easy to use, yet it implements many Machine Learning algo‐\n",
    "rithms efficiently, so it makes for a great entry point to learn Machine Learning.\n",
    "• TensorFlow is a more complex library for distributed numerical computation. It\n",
    "makes it possible to train and run very large neural networks efficiently by dis‐\n",
    "tributing the computations across potentially hundreds of multi-GPU servers.\n",
    "TensorFlow was created at Google and supports many of their large-scale\n",
    "Machine Learning applications. It was open sourced in November 2015.\n",
    "• Keras is a high level Deep Learning API that makes it very simple to train and\n",
    "run neural networks. It can run on top of either TensorFlow, Theano or Micro‐\n",
    "soft Cognitive Toolkit (formerly known as CNTK). TensorFlow comes with its\n",
    "own implementation of this API, called tf.keras, which provides support for some\n",
    "advanced TensorFlow features (e.g., to efficiently load data).\n",
    "The book favors a hands-on approach, growing an intuitive understanding of\n",
    "Machine Learning through concrete working examples and just a little bit of theory.\n",
    "While you can read this book without picking up your laptop, we highly recommend\n",
    "xii | Preface\n",
    "you experiment with the code examples available online as Jupyter notebooks at\n",
    "https://github.com/ageron/handson-ml2.\n",
    "Prerequisites\n",
    "This book assumes that you have some Python programming experience and that you\n",
    "are familiar with Python’s main scientific libraries, in particular NumPy, Pandas, and\n",
    "Matplotlib.\n",
    "Also, if you care about what’s under the hood you should have a reasonable under‐\n",
    "standing of college-level math as well (calculus, linear algebra, probabilities, and sta‐\n",
    "tistics).\n",
    "If you don’t know Python yet, http://learnpython.org/ is a great place to start. The offi‐\n",
    "cial tutorial on python.org is also quite good.\n",
    "If you have never used Jupyter, Chapter 2 will guide you through installation and the\n",
    "basics: it is a great tool to have in your toolbox.\n",
    "If you are not familiar with Python’s scientific libraries, the provided Jupyter note‐\n",
    "books include a few tutorials. There is also a quick math tutorial for linear algebra.\n",
    "Roadmap\n",
    "This book is organized in two parts. Part I, The Fundamentals of Machine Learning,\n",
    "covers the following topics:\n",
    "• What is Machine Learning? What problems does it try to solve? What are the\n",
    "main categories and fundamental concepts of Machine Learning systems?\n",
    "• The main steps in a typical Machine Learning project.\n",
    "• Learning by fitting a model to data.\n",
    "• Optimizing a cost function.\n",
    "• Handling, cleaning, and preparing data.\n",
    "• Selecting and engineering features.\n",
    "• Selecting a model and tuning hyperparameters using cross-validation.\n",
    "• The main challenges of Machine Learning, in particular underfitting and overfit‐\n",
    "ting (the bias/variance tradeoff).\n",
    "• Reducing the dimensionality of the training data to fight the curse of dimension‐\n",
    "ality.\n",
    "• Other unsupervised learning techniques, including clustering, density estimation\n",
    "and anomaly detection.\n",
    "Preface | xiii\n",
    "• The most common learning algorithms: Linear and Polynomial Regression,\n",
    "Logistic Regression, k-Nearest Neighbors, Support Vector Machines, Decision\n",
    "Trees, Random Forests, and Ensemble methods'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f08c6ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa0bf31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
